{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation:\n",
    "\n",
    "- The dataset is split into two files: True.csv and Fake.csv, each containing news articles labeled as true or fake.\n",
    "- The code reads these two CSV files and assigns a label:\n",
    "\n",
    "        True News: Label is set to 1.\n",
    "        Fake News: Label is set to 0.\n",
    "        \n",
    "- Only the relevant columns (text and label) are retained, and both datasets are concatenated to create a single DataFrame.\n",
    "- The combined dataset is shuffled to ensure random distribution of true and fake articles.\n",
    "- The train_test_split function is used to split the dataset into 80% training data and 20% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...   \n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3  FBI Russia probe helped by Australian diplomat...   \n",
      "4  Trump wants Postal Service to charge 'much mor...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
      "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
      "\n",
      "                 date  \n",
      "0  December 31, 2017   \n",
      "1  December 29, 2017   \n",
      "2  December 31, 2017   \n",
      "3  December 30, 2017   \n",
      "4  December 29, 2017   \n",
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "2  On Friday, it was revealed that former Milwauk...    News   \n",
      "3  On Christmas day, Donald Trump announced that ...    News   \n",
      "4  Pope Francis used his annual Christmas Day mes...    News   \n",
      "\n",
      "                date  \n",
      "0  December 31, 2017  \n",
      "1  December 31, 2017  \n",
      "2  December 30, 2017  \n",
      "3  December 29, 2017  \n",
      "4  December 25, 2017  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "true_news = pd.read_csv('True.csv')\n",
    "fake_news = pd.read_csv('Fake.csv')\n",
    "\n",
    "print(true_news.head())\n",
    "print(fake_news.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding label for true and fake news\n",
    "true_news['label'] = 1\n",
    "fake_news['label'] = 0\n",
    "\n",
    "# Selecting only the relevant columns\n",
    "true_news = true_news[['text', 'label']]\n",
    "fake_news = fake_news[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the two datasets\n",
    "df = pd.concat([true_news, fake_news], ignore_index=True)\n",
    "\n",
    "# Shuffling the combined DataFrame to mix the true and fake articles\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump s White House is in chaos, and th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now that Donald Trump is the presumptive GOP n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mike Pence is a huge homophobe. He supports ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAN FRANCISCO (Reuters) - California Attorney ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twisted reasoning is all that comes from Pelos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Donald Trump s White House is in chaos, and th...      0\n",
       "1  Now that Donald Trump is the presumptive GOP n...      0\n",
       "2  Mike Pence is a huge homophobe. He supports ex...      0\n",
       "3  SAN FRANCISCO (Reuters) - California Attorney ...      1\n",
       "4  Twisted reasoning is all that comes from Pelos...      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Tokenization:\n",
    "- BERT models require tokenized inputs, so the BertTokenizer is used to tokenize the text data.\n",
    "- The training and testing text data are tokenized with padding and truncation set to true, ensuring all sequences are of equal length (up to max_length=128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Loading the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenizing the text data\n",
    "train_encodings = tokenizer(list(X_train), padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "test_encodings = tokenizer(list(X_test), padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating a Custom Dataset Class:\n",
    "- A custom NewsDataset class is created to handle the encoding and labels for the dataset. This class is used to create PyTorch datasets for training and testing.\n",
    "- The  __getitem__  method returns a dictionary of encoded text items and labels for a given index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom Dataset class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Creating train and test datasets fit for PyTorch DataLoader\n",
    "train_dataset = NewsDataset(train_encodings, y_train.tolist())\n",
    "test_dataset = NewsDataset(test_encodings, y_test.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Loading and Training:\n",
    "- The pre-trained BERT model distilbert-base-uncased is loaded using the BertForSequenceClassification class, and it is set up for binary classification with num_labels=2.\n",
    "- The model is moved to the appropriate device (CPU or GPU).\n",
    "- Training arguments are defined using TrainingArguments, such as batch size, number of epochs, learning rate, and weight decay.\n",
    "- A Trainer instance is created using the model, training arguments, and datasets.\n",
    "- The trainer.train() method starts the training process, and trainer.evaluate() evaluates the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Check if model is on the correct device\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check if model is on the correct device\n",
    "model.to(device)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 35918\n",
      "Number of testing samples: 8980\n",
      "First training sample: {'input_ids': tensor([  101,  2061,  1996, 14783,  3510,  2074,  2165,  1037,  2735,  1998,\n",
      "         5163, 20147,  1010,  2145,  2725,  1996,  2694,  4984,  2000,  5326,\n",
      "         8398,  1055,  6745,  3658,  1010,  2089,  2025,  2130,  2113,  2009,\n",
      "         2664,  1012,  2096,  5163, 20147,  2001,  5697,  2183,  2006,  5830,\n",
      "         2000,  2360,  8398,  1055,  4297, 11631,  7869,  3372,  1010,  4795,\n",
      "         1056, 28394,  3215,  2323,  2022,  6439,  1998,  2111,  2323,  3579,\n",
      "         2006,  2010,  4297, 11631,  7869,  3372,  1010,  4795,  4506,  2612,\n",
      "         1010,  2014,  3129,  2018,  4593,  2584,  1037,  4911,  2391,  2007,\n",
      "         1996, 28072,  1012,  2006, 10474,  1010,  2002,  9339,  1996,  2168,\n",
      "        10041,  2594,  1056, 28394,  3215,  2010,  2564,  2001,  2006,  2694,\n",
      "         6984,  1998,  2170,  2068,  2054,  2027,  2020,  1024,  3143, 13044,\n",
      "         1012,  2577, 14783,  1055, 10474,  4070,  2003,  2025, 20119,  2021,\n",
      "        12060,  3313,  1011,  7039,  1998,  2009,  1055,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(0)}\n",
      "First testing sample: {'input_ids': tensor([  101,  2899,  1006, 26665,  1007,  1011,  1057,  1012,  1055,  1012,\n",
      "         2343,  6221,  8398,  2106,  2025,  6848,  1523,  4216,  1010,  4725,\n",
      "         2030,  2510,  3136,  1524,  1999,  2010,  3116,  2007,  2845,  3097,\n",
      "         2704, 14543,  2474, 19716,  4492,  1010,  3187,  1997,  2110, 10151,\n",
      "         6229, 18617,  2056,  2006,  6928,  1010,  2044,  4311,  2008,  8398,\n",
      "        21362,  6219,  2592,  1012,  1523,  2076,  2343,  8398,  1521,  1055,\n",
      "         3116,  2007,  3097,  2704,  2474, 19716,  4492,  1010,  1037,  5041,\n",
      "         2846,  1997,  5739,  2020,  6936,  2426,  2029,  2020,  2691,  4073,\n",
      "         1998,  8767,  4953,  4675,  1011, 10130,  1012,  2076,  2008,  3863,\n",
      "         1996,  3267,  1997,  3563,  8767,  2020,  6936,  1010,  2021,  2027,\n",
      "         2106,  2025,  6848,  4216,  1010,  4725,  2030,  2510,  3136,  1010,\n",
      "         1524,  6229, 18617,  2056,  1999,  1037,  4861,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19540\\290494850.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "\n",
    "print(f\"First training sample: {train_dataset[0]}\")\n",
    "print(f\"First testing sample: {test_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'classifier.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'classifier.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Download and cache the tokenizer and model\n",
    "BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "BertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for offline running\n",
    "import os\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer successfully loaded from the checkpoint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "#path to your checkpoint\n",
    "checkpoint_path = \"results/checkpoint-6735\"\n",
    "\n",
    "# Loading the model from the checkpoint directory\n",
    "model = BertForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Load the original tokenizer used during training\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "print(\"Model and tokenizer successfully loaded from the checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db84978620154f9da39597a6e6ed48e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19540\\290494850.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 0.4346, 'train_samples_per_second': 247923.308, 'train_steps_per_second': 15496.07, 'train_loss': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6735, training_loss=0.0, metrics={'train_runtime': 0.4346, 'train_samples_per_second': 247923.308, 'train_steps_per_second': 15496.07, 'train_loss': 0.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Resume training from the specific checkpoint\n",
    "trainer.train(resume_from_checkpoint=checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'classifier.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'classifier.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538d23e4c72646aeb387601acd756810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19540\\290494850.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred during training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:1813\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1810\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1813\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1814\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\data_loader.py:559\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[0;32m    561\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\utils\\operations.py:184\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m--> 184\u001b[0m         {\n\u001b[0;32m    185\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking, skip_keys\u001b[38;5;241m=\u001b[39mskip_keys)\n\u001b[0;32m    186\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    187\u001b[0m         }\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\utils\\operations.py:185\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m    184\u001b[0m         {\n\u001b[1;32m--> 185\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    187\u001b[0m         }\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\utils\\operations.py:156\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    154\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model.to(device)  # Move the model to the appropriate device (CPU/GPU)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,                      # Number of training epochs\n",
    "    per_device_train_batch_size=4,           # Batch size per device during training\n",
    "    per_device_eval_batch_size=4,            # Batch size per device during evaluation\n",
    "    gradient_accumulation_steps=4,           # Accumulate gradients over 4 batches\n",
    "    warmup_steps=500,                        # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                       # Weight decay to prevent overfitting\n",
    "    logging_strategy=\"steps\",                # Log at each logging step\n",
    "    logging_steps=100,                       # Log every 100 steps\n",
    "    evaluation_strategy=\"epoch\",             # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",                   # Save the model at the end of each epoch\n",
    "    report_to=[],                            # No reporting to external services\n",
    "    load_best_model_at_end=True,             # Load the best model when finished training\n",
    "    learning_rate=1e-4,                      # Set an appropriate learning rate\n",
    "    fp16=True,                               # Enable mixed precision training\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The BERT model instance\n",
    "    args=training_args,                  # Training arguments defined above\n",
    "    train_dataset=train_dataset,         # The training dataset\n",
    "    eval_dataset=test_dataset,           # The evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation:\n",
    "- Predictions are made on the test dataset using the trained model.\n",
    "- Accuracy, precision, recall, and F1-score are calculated using the sklearn metrics module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19540\\290494850.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9f68a06987413082cd598d2bbad1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071746a47e8a4de08658b316b1b49b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9976614699331848\n",
      "Precision: 0.9981421272642824\n",
      "Recall: 0.996984458362329\n",
      "F1 Score: 0.9975629569455727\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test dataset\n",
    "trainer.evaluate()\n",
    "\n",
    "# Getting predictions\n",
    "predictions = trainer.predict(test_dataset).predictions\n",
    "predictions = torch.argmax(torch.tensor(predictions), axis=1)\n",
    "\n",
    "# Calculating metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Saving and Loading the Model:\n",
    "- The trained model and tokenizer are saved locally using the save_pretrained() method, and later loaded using from_pretrained()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully at saved_model/bert_model and saved_model/bert_tokenizer\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "#Paths to save model and tokenizer\n",
    "model_save_path = \"saved_model/bert_model\"\n",
    "tokenizer_save_path = \"saved_model/bert_tokenizer\"\n",
    "\n",
    "# Saving the BERT model\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# Saving the BERT tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved successfully at {model_save_path} and {tokenizer_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "model.save_pretrained(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
